{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VAE for Chest X-Ray Generation\n",
        "**ECE 285 - Deep Generative Models**\n",
        "\n",
        "Optimized implementation with:\n",
        "- Spatial latent space for preserving structure\n",
        "- Perceptual loss (VGG) for sharp images\n",
        "- GroupNorm + SiLU for stability\n",
        "- Low beta (0.0001-0.001) for reconstruction quality\n",
        "- Two-phase training strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision import transforms, models\n",
        "from torchvision.transforms import functional as TF\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import cv2\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'img_size': 128,\n",
        "    'latent_spatial': (4, 4),\n",
        "    'latent_channels': 64,\n",
        "    'batch_size': 32,\n",
        "    'epochs_phase1': 50,\n",
        "    'epochs_phase2': 100,\n",
        "    'learning_rate': 1e-3,\n",
        "    'weight_decay': 1e-5,\n",
        "    'beta_start': 0.0,\n",
        "    'beta_end': 0.001,\n",
        "    'beta_warmup': 50,\n",
        "    'num_workers': 2,\n",
        "    'l1_weight': 10.0,\n",
        "    'perceptual_weight': 1.0,\n",
        "    'ssim_weight': 1.0,\n",
        "}\n",
        "\n",
        "DATA_DIR = '/kaggle/input/chest-xray-pneumonia/chest_xray'\n",
        "os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
        "os.makedirs('/kaggle/working/results', exist_ok=True)\n",
        "\n",
        "print('Configuration:')\n",
        "for k, v in CONFIG.items():\n",
        "    print(f'  {k}: {v}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_clahe(img_np, clip_limit=2.0, tile_size=(8, 8)):\n",
        "    if img_np.dtype != np.uint8:\n",
        "        img_np = (img_np * 255).astype(np.uint8)\n",
        "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_size)\n",
        "    return clahe.apply(img_np)\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, root_dir, img_size=128, split='train'):\n",
        "        self.img_size = img_size\n",
        "        self.is_training = (split == 'train')\n",
        "        self.image_paths = []\n",
        "        \n",
        "        split_dir = os.path.join(root_dir, split)\n",
        "        for category in ['NORMAL', 'PNEUMONIA']:\n",
        "            cat_path = os.path.join(split_dir, category)\n",
        "            if os.path.exists(cat_path):\n",
        "                for ext in ['*.jpeg', '*.jpg', '*.png']:\n",
        "                    self.image_paths.extend(glob.glob(os.path.join(cat_path, ext)))\n",
        "        \n",
        "        print(f'{split}: {len(self.image_paths)} images')\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img = Image.open(self.image_paths[idx]).convert('L')\n",
        "            img_np = apply_clahe(np.array(img))\n",
        "            img = Image.fromarray(img_np)\n",
        "            img = TF.resize(img, (self.img_size, self.img_size))\n",
        "            \n",
        "            if self.is_training and np.random.random() > 0.5:\n",
        "                img = TF.hflip(img)\n",
        "            \n",
        "            return TF.to_tensor(img)\n",
        "        except:\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "train_dataset = ChestXRayDataset(DATA_DIR, CONFIG['img_size'], 'train')\n",
        "val_dataset = ChestXRayDataset(DATA_DIR, CONFIG['img_size'], 'val')\n",
        "test_dataset = ChestXRayDataset(DATA_DIR, CONFIG['img_size'], 'test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, CONFIG['batch_size'], shuffle=True, \n",
        "                         num_workers=CONFIG['num_workers'], pin_memory=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, CONFIG['batch_size'], shuffle=False,\n",
        "                       num_workers=CONFIG['num_workers'], pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, CONFIG['batch_size'], shuffle=False,\n",
        "                        num_workers=CONFIG['num_workers'], pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize samples\n",
        "sample = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(sample[i].squeeze(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Sample Training Images')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/sample_data.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f'Stats - Mean: {sample.mean():.3f}, Std: {sample.std():.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1, groups=8):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, stride, 1)\n",
        "        self.gn1 = nn.GroupNorm(groups, out_ch)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
        "        self.gn2 = nn.GroupNorm(groups, out_ch)\n",
        "        \n",
        "        self.shortcut = nn.Identity()\n",
        "        if stride != 1 or in_ch != out_ch:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 1, stride),\n",
        "                nn.GroupNorm(groups, out_ch)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.silu(self.gn1(self.conv1(x)))\n",
        "        out = self.gn2(self.conv2(out))\n",
        "        return F.silu(out + self.shortcut(x))\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels, groups=8):\n",
        "        super().__init__()\n",
        "        self.gn = nn.GroupNorm(groups, channels)\n",
        "        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n",
        "        self.proj = nn.Conv2d(channels, channels, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.gn(x)\n",
        "        qkv = self.qkv(h).reshape(B, 3, C, H * W).permute(1, 0, 2, 3)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        \n",
        "        attn = torch.softmax(torch.bmm(q.transpose(1, 2), k) / (C ** 0.5), dim=-1)\n",
        "        out = torch.bmm(v, attn.transpose(1, 2)).reshape(B, C, H, W)\n",
        "        return x + self.proj(out)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_channels=64):\n",
        "        super().__init__()\n",
        "        self.init = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 7, 2, 3),\n",
        "            nn.GroupNorm(8, 32),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "        self.down1 = ResBlock(32, 64, 2)\n",
        "        self.down2 = ResBlock(64, 128, 2)\n",
        "        self.down3 = nn.Sequential(ResBlock(128, 256, 2), AttentionBlock(256))\n",
        "        self.down4 = ResBlock(256, 512, 2)\n",
        "        \n",
        "        self.mu_conv = nn.Conv2d(512, latent_channels, 3, 1, 1)\n",
        "        self.logvar_conv = nn.Conv2d(512, latent_channels, 3, 1, 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.init(x)\n",
        "        x = self.down1(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.down3(x)\n",
        "        x = self.down4(x)\n",
        "        return self.mu_conv(x), self.logvar_conv(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_channels=64):\n",
        "        super().__init__()\n",
        "        self.init = nn.Conv2d(latent_channels, 512, 3, 1, 1)\n",
        "        \n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ResBlock(512, 256, 1)\n",
        "        )\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ResBlock(256, 128, 1),\n",
        "            AttentionBlock(128)\n",
        "        )\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ResBlock(128, 64, 1)\n",
        "        )\n",
        "        self.up4 = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            ResBlock(64, 32, 1)\n",
        "        )\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
        "            nn.Conv2d(32, 16, 3, 1, 1),\n",
        "            nn.GroupNorm(8, 16),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(16, 1, 3, 1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, z):\n",
        "        x = self.init(z)\n",
        "        x = self.up1(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.up3(x)\n",
        "        x = self.up4(x)\n",
        "        return self.final(x)\n",
        "\n",
        "class SpatialVAE(nn.Module):\n",
        "    def __init__(self, latent_channels=64):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_channels)\n",
        "        self.decoder = Decoder(latent_channels)\n",
        "        self.latent_channels = latent_channels\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        return mu + std * torch.randn_like(std)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decoder(z), mu, logvar\n",
        "    \n",
        "    def generate(self, n, device):\n",
        "        z = torch.randn(n, self.latent_channels, 4, 4).to(device)\n",
        "        with torch.no_grad():\n",
        "            return self.decoder(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        vgg = models.vgg16(pretrained=True).features.eval().to(device)\n",
        "        for p in vgg.parameters():\n",
        "            p.requires_grad = False\n",
        "        \n",
        "        self.slices = nn.ModuleList([\n",
        "            vgg[:4],   # relu1_2\n",
        "            vgg[4:9],  # relu2_2\n",
        "            vgg[9:16]  # relu3_3\n",
        "        ])\n",
        "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        x_rgb = x.repeat(1, 3, 1, 1)\n",
        "        y_rgb = y.repeat(1, 3, 1, 1)\n",
        "        \n",
        "        # ImageNet normalization (move to device)\n",
        "        mean = self.mean.to(x.device)\n",
        "        std = self.std.to(x.device)\n",
        "        x_rgb = (x_rgb - mean) / std\n",
        "        y_rgb = (y_rgb - mean) / std\n",
        "        \n",
        "        loss = 0\n",
        "        x_feats, y_feats = x_rgb, y_rgb\n",
        "        for slice_net in self.slices:\n",
        "            x_feats = slice_net(x_feats)\n",
        "            y_feats = slice_net(y_feats)\n",
        "            loss += F.l1_loss(x_feats, y_feats)\n",
        "        return loss\n",
        "\n",
        "class SSIMLoss(nn.Module):\n",
        "    def __init__(self, window_size=11, sigma=1.5):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        coords = torch.arange(window_size, dtype=torch.float32) - window_size // 2\n",
        "        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
        "        g = g / g.sum()\n",
        "        window = (g.unsqueeze(0) * g.unsqueeze(1)).unsqueeze(0).unsqueeze(0)\n",
        "        self.register_buffer('window', window)\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        C1, C2 = 0.01 ** 2, 0.03 ** 2\n",
        "        pad = self.window_size // 2\n",
        "        window = self.window.to(x.device)\n",
        "        \n",
        "        mu_x = F.conv2d(x, window, padding=pad)\n",
        "        mu_y = F.conv2d(y, window, padding=pad)\n",
        "        mu_x_sq, mu_y_sq, mu_xy = mu_x ** 2, mu_y ** 2, mu_x * mu_y\n",
        "        \n",
        "        sigma_x_sq = F.conv2d(x * x, window, padding=pad) - mu_x_sq\n",
        "        sigma_y_sq = F.conv2d(y * y, window, padding=pad) - mu_y_sq\n",
        "        sigma_xy = F.conv2d(x * y, window, padding=pad) - mu_xy\n",
        "        \n",
        "        ssim = ((2 * mu_xy + C1) * (2 * sigma_xy + C2)) / \\\n",
        "               ((mu_x_sq + mu_y_sq + C1) * (sigma_x_sq + sigma_y_sq + C2))\n",
        "        return 1 - ssim.mean()\n",
        "\n",
        "class VAELoss(nn.Module):\n",
        "    def __init__(self, device, l1_w=1.0, perc_w=0.1, ssim_w=0.5):\n",
        "        super().__init__()\n",
        "        self.perceptual = PerceptualLoss(device)\n",
        "        self.ssim = SSIMLoss()\n",
        "        self.l1_w = l1_w\n",
        "        self.perc_w = perc_w\n",
        "        self.ssim_w = ssim_w\n",
        "    \n",
        "    def forward(self, recon, target, mu, logvar, beta=0.001):\n",
        "        l1_loss = F.l1_loss(recon, target, reduction='mean')\n",
        "        perc_loss = self.perceptual(recon, target)\n",
        "        ssim_loss = self.ssim(recon, target)\n",
        "        \n",
        "        kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        \n",
        "        recon_total = self.l1_w * l1_loss + self.perc_w * perc_loss + self.ssim_w * ssim_loss\n",
        "        total_loss = recon_total + beta * kl_loss\n",
        "        \n",
        "        return total_loss, recon_total, kl_loss, l1_loss, perc_loss, ssim_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SpatialVAE(latent_channels=CONFIG['latent_channels']).to(device)\n",
        "loss_fn = VAELoss(device, CONFIG['l1_weight'], CONFIG['perceptual_weight'], CONFIG['ssim_weight'])\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Model: SpatialVAE with GroupNorm + SiLU')\n",
        "print(f'Total parameters: {total_params:,}')\n",
        "print(f'Latent shape: {CONFIG[\"latent_spatial\"]} x {CONFIG[\"latent_channels\"]}')\n",
        "\n",
        "test_input = torch.rand(2, 1, 128, 128).to(device)\n",
        "recon, mu, logvar = model(test_input)\n",
        "print(f'Input: {test_input.shape} -> Output: {recon.shape}')\n",
        "print(f'Latent mu: {mu.shape}')\n",
        "print(f'Output range: [{recon.min():.3f}, {recon.max():.3f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_beta(epoch, phase, config):\n",
        "    if phase == 1:\n",
        "        return 0.0\n",
        "    if epoch >= config['beta_warmup']:\n",
        "        return config['beta_end']\n",
        "    return config['beta_start'] + (config['beta_end'] - config['beta_start']) * (epoch / config['beta_warmup'])\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, device, beta):\n",
        "    model.train()\n",
        "    total_loss = total_recon = total_kl = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for data in pbar:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon, mu, logvar = model(data)\n",
        "        loss, recon_loss, kl_loss, l1, perc, ssim = loss_fn(recon, data, mu, logvar, beta)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_recon += recon_loss.item()\n",
        "        total_kl += kl_loss.item()\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{loss.item():.2f}', 'recon': f'{recon_loss.item():.2f}'})\n",
        "    \n",
        "    n = len(loader)\n",
        "    return total_loss/n, total_recon/n, total_kl/n\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, loss_fn, device, beta):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        recon, mu, logvar = model(data)\n",
        "        loss, _, _, _, _, _ = loss_fn(recon, data, mu, logvar, beta)\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Reconstruction Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
        "\n",
        "train_losses, recon_losses, kl_losses, val_losses, beta_history = [], [], [], [], []\n",
        "\n",
        "print('='*60)\n",
        "print('PHASE 1: Reconstruction-Only Training (beta=0)')\n",
        "print('='*60)\n",
        "\n",
        "for epoch in range(1, CONFIG['epochs_phase1'] + 1):\n",
        "    beta = get_beta(epoch, 1, CONFIG)\n",
        "    beta_history.append(beta)\n",
        "    \n",
        "    train_loss, recon_loss, kl_loss = train_epoch(model, train_loader, optimizer, loss_fn, device, beta)\n",
        "    val_loss = validate(model, val_loader, loss_fn, device, beta)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    recon_losses.append(recon_loss)\n",
        "    kl_losses.append(kl_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    scheduler.step()\n",
        "    \n",
        "    print(f'Epoch {epoch}/{CONFIG[\"epochs_phase1\"]} | Train: {train_loss:.3f} | Val: {val_loss:.3f}')\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        samples = model.generate(4, device)\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
        "        for i, ax in enumerate(axes):\n",
        "            ax.imshow(samples[i].cpu().squeeze(), cmap='gray')\n",
        "            ax.axis('off')\n",
        "        plt.suptitle(f'Phase 1 - Epoch {epoch}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "torch.save(model.state_dict(), '/kaggle/working/checkpoints/phase1_final.pt')\n",
        "print('Phase 1 complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: VAE Training with KL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'] * 0.5, weight_decay=CONFIG['weight_decay'])\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', 0.5, 10, verbose=True)\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('PHASE 2: VAE Training with KL Regularization')\n",
        "print('='*60)\n",
        "\n",
        "best_loss = float('inf')\n",
        "phase2_start = CONFIG['epochs_phase1']\n",
        "\n",
        "for epoch in range(1, CONFIG['epochs_phase2'] + 1):\n",
        "    global_epoch = phase2_start + epoch\n",
        "    beta = get_beta(epoch, 2, CONFIG)\n",
        "    beta_history.append(beta)\n",
        "    \n",
        "    train_loss, recon_loss, kl_loss = train_epoch(model, train_loader, optimizer, loss_fn, device, beta)\n",
        "    val_loss = validate(model, val_loader, loss_fn, device, beta)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    recon_losses.append(recon_loss)\n",
        "    kl_losses.append(kl_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    print(f'Epoch {global_epoch} | Î²: {beta:.5f} | Train: {train_loss:.3f} | Val: {val_loss:.3f} | KL: {kl_loss:.3f}')\n",
        "    \n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': global_epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'config': CONFIG,\n",
        "        }, '/kaggle/working/checkpoints/best_model.pt')\n",
        "        print(f'  -> Saved best model (val_loss: {val_loss:.3f})')\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "        samples = model.generate(8, device)\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            ax.imshow(samples[i].cpu().squeeze(), cmap='gray')\n",
        "            ax.axis('off')\n",
        "        plt.suptitle(f'Phase 2 - Epoch {global_epoch} (Î²={beta:.5f})')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "torch.save(model.state_dict(), '/kaggle/working/checkpoints/final_model.pt')\n",
        "print('\\nTraining complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Curves & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "axes[0, 0].plot(train_losses, label='Train', linewidth=2)\n",
        "axes[0, 0].plot(val_losses, label='Val', linewidth=2)\n",
        "axes[0, 0].axvline(CONFIG['epochs_phase1'], color='red', linestyle='--', alpha=0.5, label='Phase 2')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Total Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "axes[0, 1].plot(recon_losses, linewidth=2, color='green')\n",
        "axes[0, 1].axvline(CONFIG['epochs_phase1'], color='red', linestyle='--', alpha=0.5)\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].set_title('Reconstruction Loss')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "axes[1, 0].plot(kl_losses, linewidth=2, color='red')\n",
        "axes[1, 0].axvline(CONFIG['epochs_phase1'], color='red', linestyle='--', alpha=0.5)\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].set_title('KL Divergence')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "axes[1, 1].plot(beta_history, linewidth=2, color='purple')\n",
        "axes[1, 1].axvline(CONFIG['epochs_phase1'], color='red', linestyle='--', alpha=0.5)\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Beta')\n",
        "axes[1, 1].set_title('Beta Schedule')\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/loss_curves.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f'Final Train Loss: {train_losses[-1]:.3f}')\n",
        "print(f'Final Val Loss: {val_losses[-1]:.3f}')\n",
        "print(f'Best Val Loss: {best_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load('/kaggle/working/checkpoints/best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(f'Loaded best model from epoch {checkpoint[\"epoch\"]}')\n",
        "\n",
        "# Generated samples\n",
        "generated = model.generate(16, device)\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(generated[i].cpu().squeeze(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Generated Chest X-Ray Images', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/generated_samples.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "# Reconstructions\n",
        "test_batch = next(iter(test_loader))[:8].to(device)\n",
        "with torch.no_grad():\n",
        "    recon, _, _ = model(test_batch)\n",
        "\n",
        "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
        "for i in range(8):\n",
        "    axes[0, i].imshow(test_batch[i].cpu().squeeze(), cmap='gray')\n",
        "    axes[0, i].axis('off')\n",
        "    axes[1, i].imshow(recon[i].cpu().squeeze(), cmap='gray')\n",
        "    axes[1, i].axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/reconstructions.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary = f\"\"\"\n",
        "VAE Chest X-Ray Generation - Results Summary\n",
        "{'='*60}\n",
        "\n",
        "ARCHITECTURE:\n",
        "- Model: Spatial VAE (GroupNorm + SiLU)\n",
        "- Latent: {CONFIG['latent_spatial']} x {CONFIG['latent_channels']} channels\n",
        "- Parameters: {total_params:,}\n",
        "\n",
        "LOSS FUNCTION:\n",
        "- L1 Loss (weight: {CONFIG['l1_weight']})\n",
        "- Perceptual Loss (VGG, weight: {CONFIG['perceptual_weight']})\n",
        "- SSIM Loss (weight: {CONFIG['ssim_weight']})\n",
        "- KL Divergence (beta: {CONFIG['beta_start']}->{CONFIG['beta_end']})\n",
        "\n",
        "TRAINING:\n",
        "- Phase 1: {CONFIG['epochs_phase1']} epochs (reconstruction only)\n",
        "- Phase 2: {CONFIG['epochs_phase2']} epochs (with KL)\n",
        "- Learning Rate: {CONFIG['learning_rate']}\n",
        "\n",
        "RESULTS:\n",
        "- Final Train Loss: {train_losses[-1]:.3f}\n",
        "- Final Val Loss: {val_losses[-1]:.3f}\n",
        "- Best Val Loss: {best_loss:.3f}\n",
        "\n",
        "KEY IMPROVEMENTS:\n",
        "1. Spatial latent space (preserves structure)\n",
        "2. Very low beta (0.001 for sharp images)\n",
        "3. Perceptual loss (VGG features)\n",
        "4. L1 loss instead of MSE\n",
        "5. Two-phase training strategy\n",
        "6. GroupNorm + SiLU for stability\n",
        "\"\"\"\n",
        "\n",
        "with open('/kaggle/working/results/summary.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(summary)\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('Training Complete!')\n",
        "print('='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation - FID & Inception Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InceptionFeatures(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "        self.blocks = nn.Sequential(\n",
        "            inception.Conv2d_1a_3x3, inception.Conv2d_2a_3x3, inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(3, 2), inception.Conv2d_3b_1x1, inception.Conv2d_4a_3x3,\n",
        "            nn.MaxPool2d(3, 2), inception.Mixed_5b, inception.Mixed_5c, inception.Mixed_5d,\n",
        "            inception.Mixed_6a, inception.Mixed_6b, inception.Mixed_6c, inception.Mixed_6d,\n",
        "            inception.Mixed_6e, inception.Mixed_7a, inception.Mixed_7b, inception.Mixed_7c,\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.to(device).eval()\n",
        "        self.device = device\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def forward(self, x):\n",
        "        if x.size(1) == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "        x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "        return self.blocks(x).view(x.size(0), -1)\n",
        "\n",
        "def get_activations(images, feat_model, batch_size=32):\n",
        "    activations = []\n",
        "    loader = DataLoader(TensorDataset(images), batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    for batch in tqdm(loader, desc='Extracting features'):\n",
        "        act = feat_model(batch[0].to(feat_model.device))\n",
        "        activations.append(act.cpu().numpy())\n",
        "    \n",
        "    return np.concatenate(activations, axis=0)\n",
        "\n",
        "def calculate_fid(real_acts, fake_acts):\n",
        "    mu_r, mu_f = np.mean(real_acts, axis=0), np.mean(fake_acts, axis=0)\n",
        "    sigma_r = np.cov(real_acts, rowvar=False)\n",
        "    sigma_f = np.cov(fake_acts, rowvar=False)\n",
        "    \n",
        "    diff = mu_r - mu_f\n",
        "    covmean, _ = linalg.sqrtm(sigma_r @ sigma_f, disp=False)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "    \n",
        "    fid = diff @ diff + np.trace(sigma_r + sigma_f - 2 * covmean)\n",
        "    return float(fid)\n",
        "\n",
        "def calculate_inception_score(images, device, batch_size=32, splits=10):\n",
        "    inception = models.inception_v3(pretrained=True).to(device).eval()\n",
        "    preds = []\n",
        "    \n",
        "    loader = DataLoader(TensorDataset(images), batch_size=batch_size, shuffle=False)\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='Computing Inception Score'):\n",
        "            x = batch[0].to(device)\n",
        "            if x.size(1) == 1:\n",
        "                x = x.repeat(1, 3, 1, 1)\n",
        "            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "            pred = F.softmax(inception(x), dim=1)\n",
        "            preds.append(pred.cpu().numpy())\n",
        "    \n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    scores = []\n",
        "    split_size = preds.shape[0] // splits\n",
        "    \n",
        "    for i in range(splits):\n",
        "        part = preds[i * split_size:(i + 1) * split_size]\n",
        "        py = np.mean(part, axis=0)\n",
        "        kl = part * (np.log(part + 1e-10) - np.log(py + 1e-10))\n",
        "        scores.append(np.exp(np.mean(np.sum(kl, axis=1))))\n",
        "    \n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "print('Evaluation functions loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare evaluation data\n",
        "NUM_EVAL = 1000\n",
        "print(f'Preparing {NUM_EVAL} images for evaluation...')\n",
        "\n",
        "# Generate fake images\n",
        "print('\\nGenerating synthetic images...')\n",
        "fake_imgs = model.generate(NUM_EVAL, device).cpu()\n",
        "\n",
        "# Collect real images\n",
        "print('Collecting real test images...')\n",
        "real_imgs_list = []\n",
        "for batch in test_loader:\n",
        "    real_imgs_list.append(batch)\n",
        "    if len(real_imgs_list) * batch.size(0) >= NUM_EVAL:\n",
        "        break\n",
        "real_imgs = torch.cat(real_imgs_list, dim=0)[:NUM_EVAL]\n",
        "\n",
        "print(f'\\nReal images: {real_imgs.shape}, range [{real_imgs.min():.3f}, {real_imgs.max():.3f}]')\n",
        "print(f'Fake images: {fake_imgs.shape}, range [{fake_imgs.min():.3f}, {fake_imgs.max():.3f}]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate FID Score\n",
        "print('\\n' + '='*60)\n",
        "print('Computing FID Score')\n",
        "print('='*60)\n",
        "\n",
        "try:\n",
        "    inception_model = InceptionFeatures(device)\n",
        "    print('Extracting features from real images...')\n",
        "    real_acts = get_activations(real_imgs, inception_model)\n",
        "    print('Extracting features from generated images...')\n",
        "    fake_acts = get_activations(fake_imgs, inception_model)\n",
        "    \n",
        "    fid_score = calculate_fid(real_acts, fake_acts)\n",
        "    print(f'\\nâœ“ FID Score: {fid_score:.2f}')\n",
        "    \n",
        "    # Interpretation\n",
        "    if fid_score < 50:\n",
        "        print('  â†’ Excellent quality!')\n",
        "    elif fid_score < 100:\n",
        "        print('  â†’ Good quality')\n",
        "    elif fid_score < 200:\n",
        "        print('  â†’ Moderate quality')\n",
        "    else:\n",
        "        print('  â†’ Needs improvement')\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f'âœ— FID calculation failed: {e}')\n",
        "    fid_score = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Inception Score\n",
        "print('\\n' + '='*60)\n",
        "print('Computing Inception Score')\n",
        "print('='*60)\n",
        "\n",
        "try:\n",
        "    is_mean, is_std = calculate_inception_score(fake_imgs, device)\n",
        "    print(f'\\nâœ“ Inception Score: {is_mean:.3f} Â± {is_std:.3f}')\n",
        "    \n",
        "    # Interpretation\n",
        "    if is_mean > 5:\n",
        "        print('  â†’ Excellent diversity and quality!')\n",
        "    elif is_mean > 3:\n",
        "        print('  â†’ Good diversity')\n",
        "    elif is_mean > 2:\n",
        "        print('  â†’ Moderate diversity')\n",
        "    else:\n",
        "        print('  â†’ Low diversity')\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f'âœ— Inception Score calculation failed: {e}')\n",
        "    is_mean, is_std = None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Evaluation Summary\n",
        "print('\\n' + '='*70)\n",
        "print(' '*20 + 'EVALUATION SUMMARY')\n",
        "print('='*70)\n",
        "\n",
        "print('\\nMODEL ARCHITECTURE:')\n",
        "print(f'  â€¢ Total Parameters: {total_params:,}')\n",
        "print(f'  â€¢ Latent Space: {CONFIG[\"latent_spatial\"]} Ã— {CONFIG[\"latent_channels\"]} channels')\n",
        "print(f'  â€¢ Architecture: Spatial VAE with GroupNorm + SiLU + Attention')\n",
        "\n",
        "print('\\nTRAINING RESULTS:')\n",
        "print(f'  â€¢ Best Validation Loss: {best_loss:.4f}')\n",
        "print(f'  â€¢ Final Train Loss: {train_losses[-1]:.4f}')\n",
        "print(f'  â€¢ Final Val Loss: {val_losses[-1]:.4f}')\n",
        "print(f'  â€¢ Total Epochs: {len(train_losses)}')\n",
        "\n",
        "print('\\nGENERATIVE METRICS:')\n",
        "if fid_score:\n",
        "    print(f'  â€¢ FID Score: {fid_score:.2f}')\n",
        "    fid_status = 'Excellent' if fid_score < 50 else 'Good' if fid_score < 100 else 'Moderate' if fid_score < 200 else 'Poor'\n",
        "    print(f'    Status: {fid_status}')\n",
        "else:\n",
        "    print('  â€¢ FID Score: N/A')\n",
        "\n",
        "if is_mean:\n",
        "    print(f'  â€¢ Inception Score: {is_mean:.3f} Â± {is_std:.3f}')\n",
        "    is_status = 'Excellent' if is_mean > 5 else 'Good' if is_mean > 3 else 'Moderate' if is_mean > 2 else 'Low'\n",
        "    print(f'    Status: {is_status}')\n",
        "else:\n",
        "    print('  â€¢ Inception Score: N/A')\n",
        "\n",
        "print('\\nLOSS CONFIGURATION:')\n",
        "print(f'  â€¢ L1 Weight: {CONFIG[\"l1_weight\"]}')\n",
        "print(f'  â€¢ Perceptual Weight: {CONFIG[\"perceptual_weight\"]}')\n",
        "print(f'  â€¢ SSIM Weight: {CONFIG[\"ssim_weight\"]}')\n",
        "print(f'  â€¢ Beta Range: {CONFIG[\"beta_start\"]} â†’ {CONFIG[\"beta_end\"]}')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('Evaluation Complete!')\n",
        "print('='*70)\n",
        "\n",
        "# Save results to file\n",
        "results_dict = {\n",
        "    'model_params': total_params,\n",
        "    'latent_dim': CONFIG['latent_channels'] * 4 * 4,\n",
        "    'best_val_loss': float(best_loss),\n",
        "    'final_train_loss': float(train_losses[-1]),\n",
        "    'final_val_loss': float(val_losses[-1]),\n",
        "    'fid_score': float(fid_score) if fid_score else None,\n",
        "    'inception_mean': float(is_mean) if is_mean else None,\n",
        "    'inception_std': float(is_std) if is_std else None,\n",
        "    'config': CONFIG\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('/kaggle/working/results/evaluation_metrics.json', 'w') as f:\n",
        "    json.dump(results_dict, f, indent=2)\n",
        "\n",
        "print('\\nâœ“ Saved metrics to: /kaggle/working/results/evaluation_metrics.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Reconstruction Quality Fixes Applied\n",
        "\n",
        "**Key Changes:**\n",
        "1. **Loss Normalization**: Changed from `sum` to `mean` reduction\n",
        "2. **Perceptual Loss**: Added ImageNet normalization\n",
        "3. **Learning Rate**: Increased from `1e-4` to `1e-3` for Phase 1\n",
        "4. **Loss Weights**: \n",
        "   - L1: `1.0` â†’ `10.0`\n",
        "   - Perceptual: `0.1` â†’ `1.0`\n",
        "   - SSIM: `0.5` â†’ `1.0`\n",
        "5. **Training Loop**: Removed redundant division\n",
        "\n",
        "**Expected Results:**\n",
        "- Loss values should be in range [0.1 - 2.0] instead of [800-900]\n",
        "- Sharper reconstructions with better contrast\n",
        "- Faster convergence in Phase 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: Test reconstruction quality before training\n",
        "print('='*60)\n",
        "print('DIAGNOSTIC TEST - Reconstruction Quality')\n",
        "print('='*60)\n",
        "\n",
        "model_test = SpatialVAE(latent_channels=CONFIG['latent_channels']).to(device)\n",
        "loss_fn_test = VAELoss(device, CONFIG['l1_weight'], CONFIG['perceptual_weight'], CONFIG['ssim_weight'])\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "sample_batch = sample_batch.to(device)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    recon, mu, logvar = model_test(sample_batch)\n",
        "    loss, recon_loss, kl_loss, l1, perc, ssim = loss_fn_test(recon, sample_batch, mu, logvar, beta=0.0)\n",
        "\n",
        "print(f'\\nUntrained Model Loss Check:')\n",
        "print(f'  Total Loss: {loss.item():.4f}')\n",
        "print(f'  Reconstruction Loss: {recon_loss.item():.4f}')\n",
        "print(f'  L1 Loss: {l1.item():.4f}')\n",
        "print(f'  Perceptual Loss: {perc.item():.4f}')\n",
        "print(f'  SSIM Loss: {ssim.item():.4f}')\n",
        "\n",
        "print(f'\\nReconstruction Stats:')\n",
        "print(f'  Input range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]')\n",
        "print(f'  Output range: [{recon.min():.3f}, {recon.max():.3f}]')\n",
        "print(f'  Mean absolute diff: {(sample_batch - recon).abs().mean():.4f}')\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "for i in range(4):\n",
        "    axes[0, i].imshow(sample_batch[i].cpu().squeeze(), cmap='gray')\n",
        "    axes[0, i].set_title('Original')\n",
        "    axes[0, i].axis('off')\n",
        "    axes[1, i].imshow(recon[i].cpu().squeeze(), cmap='gray')\n",
        "    axes[1, i].set_title(f'Recon (untrained)')\n",
        "    axes[1, i].axis('off')\n",
        "plt.suptitle('Reconstruction Test - Untrained Model')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nâœ“ Loss values are now normalized (should be < 5.0)')\n",
        "print('âœ“ Ready to start training with improved configuration!')\n",
        "print('='*60)\n",
        "\n",
        "del model_test, loss_fn_test\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
