{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VAE for Chest X-Ray Image Generation\n",
        "ECE 285 - Deep Generative Models Assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "%pip install -q scipy tqdm matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "CONFIG = {\n",
        "    'img_size': 128,\n",
        "    'latent_dim': 512,\n",
        "    'batch_size': 32,\n",
        "    'epochs': 100,\n",
        "    'learning_rate': 1e-4,\n",
        "    'beta_start': 0.0,      # Start with no KL penalty (focus on reconstruction)\n",
        "    'beta_end': 1.0,        # End with full KL penalty\n",
        "    'beta_warmup_epochs': 70,  # Epochs to linearly increase beta from start to end\n",
        "    'num_workers': 2,\n",
        "}\n",
        "\n",
        "def get_beta(epoch, config):\n",
        "    # Linear annealing: beta increases from beta_start to beta_end over warmup_epochs\n",
        "    if epoch >= config['beta_warmup_epochs']:\n",
        "        return config['beta_end']\n",
        "    return config['beta_start'] + (config['beta_end'] - config['beta_start']) * (epoch / config['beta_warmup_epochs'])\n",
        "\n",
        "# Kaggle dataset path\n",
        "DATA_DIR = '/kaggle/input/chest-xray-pneumonia/chest_xray'\n",
        "\n",
        "# Output directories\n",
        "os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n",
        "os.makedirs('/kaggle/working/results', exist_ok=True)\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, root_dir, img_size=128, split='train'):\n",
        "        self.img_size = img_size\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        \n",
        "        self.image_paths = []\n",
        "        split_dir = os.path.join(root_dir, split)\n",
        "        \n",
        "        if not os.path.exists(split_dir):\n",
        "            raise FileNotFoundError(f\"Directory not found: {split_dir}\")\n",
        "        \n",
        "        for category in ['NORMAL', 'PNEUMONIA']:\n",
        "            category_path = os.path.join(split_dir, category)\n",
        "            if os.path.exists(category_path):\n",
        "                patterns = ['*.jpeg', '*.jpg', '*.png', '*.JPEG', '*.JPG', '*.PNG']\n",
        "                for pattern in patterns:\n",
        "                    self.image_paths.extend(glob.glob(os.path.join(category_path, pattern)))\n",
        "        \n",
        "        if len(self.image_paths) == 0:\n",
        "            raise ValueError(f\"No images found in {split_dir}\")\n",
        "        \n",
        "        print(f\"Found {len(self.image_paths)} images in {split} split\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            img_path = self.image_paths[idx]\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image = self.transform(image)\n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
        "            return self.__getitem__((idx + 1) % len(self))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets and dataloaders\n",
        "train_dataset = ChestXRayDataset(DATA_DIR, CONFIG['img_size'], split='train')\n",
        "val_dataset = ChestXRayDataset(DATA_DIR, CONFIG['img_size'], split='val')\n",
        "test_dataset = ChestXRayDataset(DATA_DIR, CONFIG['img_size'], split='test')\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=CONFIG['batch_size'], shuffle=True,\n",
        "    num_workers=CONFIG['num_workers'], pin_memory=True, drop_last=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'], pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
        "    num_workers=CONFIG['num_workers'], pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images\n",
        "sample_batch = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(sample_batch[i].squeeze().numpy(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Sample Training Images', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/sample_data.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. VAE Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim=256, img_channels=1):\n",
        "        super().__init__()\n",
        "        # Conv layers: 128x128 -> 64 -> 32 -> 16 -> 8 -> 4\n",
        "        self.conv1 = nn.Conv2d(img_channels, 32, 4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2, padding=1)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 4, stride=2, padding=1)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        \n",
        "        # Flatten: 512 * 4 * 4 = 8192\n",
        "        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
        "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
        "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
        "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2)\n",
        "        x = F.leaky_relu(self.bn5(self.conv5(x)), 0.2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=256, img_channels=1):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 512 * 4 * 4)\n",
        "        \n",
        "        # Transposed conv: 4 -> 8 -> 16 -> 32 -> 64 -> 128\n",
        "        self.deconv1 = nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
        "        self.deconv4 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)\n",
        "        self.deconv5 = nn.ConvTranspose2d(32, img_channels, 4, stride=2, padding=1)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(256)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "    \n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = x.view(x.size(0), 512, 4, 4)\n",
        "        x = F.relu(self.bn1(self.deconv1(x)))\n",
        "        x = F.relu(self.bn2(self.deconv2(x)))\n",
        "        x = F.relu(self.bn3(self.deconv3(x)))\n",
        "        x = F.relu(self.bn4(self.deconv4(x)))\n",
        "        x = torch.sigmoid(self.deconv5(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim=256, img_channels=1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_dim, img_channels)\n",
        "        self.decoder = Decoder(latent_dim, img_channels)\n",
        "        self.latent_dim = latent_dim\n",
        "    \n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # z = mu + std * eps\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decoder(z)\n",
        "        return recon, mu, logvar\n",
        "    \n",
        "    def generate(self, num_samples, device):\n",
        "        z = torch.randn(num_samples, self.latent_dim).to(device)\n",
        "        with torch.no_grad():\n",
        "            samples = self.decoder(z)\n",
        "        return samples\n",
        "\n",
        "\n",
        "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
        "    # Reconstruction loss (BCE)\n",
        "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    # KL divergence\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = VAE(latent_dim=CONFIG['latent_dim'], img_channels=1).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(2, 1, 128, 128).to(device)\n",
        "recon, mu, logvar = model(test_input)\n",
        "print(f\"\\nInput shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {recon.shape}\")\n",
        "print(f\"Latent mu shape: {mu.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training setup\n",
        "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Loss tracking\n",
        "train_losses = []\n",
        "recon_losses = []\n",
        "kl_losses = []\n",
        "val_losses = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, device, beta):\n",
        "    model.train()\n",
        "    total_loss, total_recon, total_kl = 0, 0, 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc='Training')\n",
        "    for batch_idx, data in enumerate(pbar):\n",
        "        try:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            recon, mu, logvar = model(data)\n",
        "            loss, recon_loss, kl_loss = vae_loss(recon, data, mu, logvar, beta)\n",
        "            \n",
        "            # Normalize by batch size\n",
        "            loss = loss / data.size(0)\n",
        "            recon_loss = recon_loss / data.size(0)\n",
        "            kl_loss = kl_loss / data.size(0)\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_recon += recon_loss.item()\n",
        "            total_kl += kl_loss.item()\n",
        "            \n",
        "            pbar.set_postfix({'loss': f'{loss.item():.2f}', 'recon': f'{recon_loss.item():.2f}', 'kl': f'{kl_loss.item():.2f}'})\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    n = len(train_loader)\n",
        "    return total_loss/n, total_recon/n, total_kl/n\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, val_loader, device, beta):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for data in val_loader:\n",
        "        data = data.to(device)\n",
        "        recon, mu, logvar = model(data)\n",
        "        loss, _, _ = vae_loss(recon, data, mu, logvar, beta)\n",
        "        total_loss += loss.item() / data.size(0)\n",
        "    \n",
        "    return total_loss / len(val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop with beta annealing\n",
        "best_loss = float('inf')\n",
        "beta_history = []  # Track beta values\n",
        "\n",
        "for epoch in range(1, CONFIG['epochs'] + 1):\n",
        "    # Calculate annealed beta for this epoch\n",
        "    current_beta = get_beta(epoch, CONFIG)\n",
        "    beta_history.append(current_beta)\n",
        "    \n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch}/{CONFIG['epochs']} | Beta: {current_beta:.4f}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    # Train with current beta\n",
        "    train_loss, recon_loss, kl_loss = train_epoch(model, train_loader, optimizer, device, current_beta)\n",
        "    train_losses.append(train_loss)\n",
        "    recon_losses.append(recon_loss)\n",
        "    kl_losses.append(kl_loss)\n",
        "    \n",
        "    # Validate with current beta\n",
        "    val_loss = validate(model, val_loader, device, current_beta)\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    print(f\"Train Loss: {train_loss:.4f} | Recon: {recon_loss:.4f} | KL: {kl_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_loss)\n",
        "    \n",
        "    # Save best model (after warmup period for fair comparison)\n",
        "    if epoch > CONFIG['beta_warmup_epochs'] and val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'beta_history': beta_history,\n",
        "        }, '/kaggle/working/checkpoints/best_model.pt')\n",
        "        print(f\"Saved best model (val_loss: {val_loss:.4f})\")\n",
        "    \n",
        "    # Save checkpoint every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(model.state_dict(), f'/kaggle/working/checkpoints/checkpoint_epoch_{epoch}.pt')\n",
        "        \n",
        "        # Generate samples\n",
        "        samples = model.generate(4, device)\n",
        "        fig, axes = plt.subplots(1, 4, figsize=(10, 3))\n",
        "        for i, ax in enumerate(axes):\n",
        "            ax.imshow(samples[i].cpu().squeeze().numpy(), cmap='gray')\n",
        "            ax.axis('off')\n",
        "        plt.suptitle(f'Generated Samples - Epoch {epoch} (β={current_beta:.2f})')\n",
        "        plt.savefig(f'/kaggle/working/results/samples_epoch_{epoch}.png', dpi=150)\n",
        "        plt.show()\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), '/kaggle/working/checkpoints/final_model.pt')\n",
        "print(\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Loss Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves with beta annealing\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Total loss\n",
        "axes[0, 0].plot(train_losses, label='Train', color='blue', linewidth=2)\n",
        "axes[0, 0].plot(val_losses, label='Validation', color='orange', linewidth=2)\n",
        "axes[0, 0].axvline(x=CONFIG['beta_warmup_epochs'], color='gray', linestyle='--', alpha=0.7, label='Warmup End')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Total Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Reconstruction loss\n",
        "axes[0, 1].plot(recon_losses, color='green', linewidth=2)\n",
        "axes[0, 1].axvline(x=CONFIG['beta_warmup_epochs'], color='gray', linestyle='--', alpha=0.7)\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('Loss')\n",
        "axes[0, 1].set_title('Reconstruction Loss')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# KL loss\n",
        "axes[1, 0].plot(kl_losses, color='red', linewidth=2)\n",
        "axes[1, 0].axvline(x=CONFIG['beta_warmup_epochs'], color='gray', linestyle='--', alpha=0.7)\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Loss')\n",
        "axes[1, 0].set_title('KL Divergence Loss')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Beta annealing schedule\n",
        "axes[1, 1].plot(beta_history, color='purple', linewidth=2)\n",
        "axes[1, 1].axvline(x=CONFIG['beta_warmup_epochs'], color='gray', linestyle='--', alpha=0.7, label='Warmup End')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Beta (β)')\n",
        "axes[1, 1].set_title('Beta Annealing Schedule')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/loss_curves.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal Train Loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final Val Loss: {val_losses[-1]:.4f}\")\n",
        "print(f\"Best Val Loss: {best_loss:.4f}\")\n",
        "print(f\"Final Beta: {beta_history[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load('/kaggle/working/checkpoints/best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate samples\n",
        "num_samples = 16\n",
        "generated_images = model.generate(num_samples, device)\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(generated_images[i].cpu().squeeze().numpy(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.suptitle('Generated Chest X-Ray Images', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/generated_samples.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reconstructions\n",
        "model.eval()\n",
        "sample_batch = next(iter(test_loader))[:8].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    recon, _, _ = model(sample_batch)\n",
        "\n",
        "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
        "for i in range(8):\n",
        "    axes[0, i].imshow(sample_batch[i].cpu().squeeze().numpy(), cmap='gray')\n",
        "    axes[0, i].axis('off')\n",
        "    if i == 0:\n",
        "        axes[0, i].set_title('Original', fontsize=10)\n",
        "    \n",
        "    axes[1, i].imshow(recon[i].cpu().squeeze().numpy(), cmap='gray')\n",
        "    axes[1, i].axis('off')\n",
        "    if i == 0:\n",
        "        axes[1, i].set_title('Reconstructed', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/reconstructions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Latent space interpolation\n",
        "model.eval()\n",
        "num_steps = 10\n",
        "\n",
        "z1 = torch.randn(1, CONFIG['latent_dim']).to(device)\n",
        "z2 = torch.randn(1, CONFIG['latent_dim']).to(device)\n",
        "\n",
        "interpolations = []\n",
        "for alpha in np.linspace(0, 1, num_steps):\n",
        "    z = (1 - alpha) * z1 + alpha * z2\n",
        "    with torch.no_grad():\n",
        "        img = model.decoder(z)\n",
        "    interpolations.append(img)\n",
        "\n",
        "fig, axes = plt.subplots(1, num_steps, figsize=(num_steps * 1.5, 2))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(interpolations[i].cpu().squeeze().numpy(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Latent Space Interpolation', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('/kaggle/working/results/interpolation.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation (FID & Inception Score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InceptionV3Features(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "        self.blocks = nn.Sequential(\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            inception.Conv2d_3b_1x1,\n",
        "            inception.Conv2d_4a_3x3,\n",
        "            nn.MaxPool2d(3, stride=2),\n",
        "            inception.Mixed_5b,\n",
        "            inception.Mixed_5c,\n",
        "            inception.Mixed_5d,\n",
        "            inception.Mixed_6a,\n",
        "            inception.Mixed_6b,\n",
        "            inception.Mixed_6c,\n",
        "            inception.Mixed_6d,\n",
        "            inception.Mixed_6e,\n",
        "            inception.Mixed_7a,\n",
        "            inception.Mixed_7b,\n",
        "            inception.Mixed_7c,\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.to(device)\n",
        "        self.eval()\n",
        "        self.device = device\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def forward(self, x):\n",
        "        if x.size(1) == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "        x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "        x = self.blocks(x)\n",
        "        return x.view(x.size(0), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_activations(images, model, batch_size=32):\n",
        "    model.eval()\n",
        "    activations = []\n",
        "    \n",
        "    dataset = TensorDataset(images)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    for batch in tqdm(loader, desc='Computing activations'):\n",
        "        batch = batch[0].to(model.device)\n",
        "        act = model(batch)\n",
        "        activations.append(act.cpu().numpy())\n",
        "    \n",
        "    return np.concatenate(activations, axis=0)\n",
        "\n",
        "\n",
        "def calculate_fid(real_activations, fake_activations):\n",
        "    mu_real = np.mean(real_activations, axis=0)\n",
        "    mu_fake = np.mean(fake_activations, axis=0)\n",
        "    sigma_real = np.cov(real_activations, rowvar=False)\n",
        "    sigma_fake = np.cov(fake_activations, rowvar=False)\n",
        "    \n",
        "    diff = mu_real - mu_fake\n",
        "    \n",
        "    try:\n",
        "        covmean, _ = linalg.sqrtm(sigma_real @ sigma_fake, disp=False)\n",
        "        if np.iscomplexobj(covmean):\n",
        "            covmean = covmean.real\n",
        "        fid = diff @ diff + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
        "    except Exception as e:\n",
        "        print(f\"FID calculation error: {e}\")\n",
        "        fid = float('inf')\n",
        "    \n",
        "    return fid\n",
        "\n",
        "\n",
        "def calculate_inception_score(images, device, batch_size=32, splits=10):\n",
        "    inception = models.inception_v3(pretrained=True).to(device)\n",
        "    inception.eval()\n",
        "    \n",
        "    preds = []\n",
        "    dataset = TensorDataset(images)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='Computing IS'):\n",
        "            batch = batch[0].to(device)\n",
        "            if batch.size(1) == 1:\n",
        "                batch = batch.repeat(1, 3, 1, 1)\n",
        "            batch = F.interpolate(batch, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "            pred = F.softmax(inception(batch), dim=1)\n",
        "            preds.append(pred.cpu().numpy())\n",
        "    \n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    \n",
        "    scores = []\n",
        "    split_size = preds.shape[0] // splits\n",
        "    \n",
        "    for i in range(splits):\n",
        "        part = preds[i * split_size:(i + 1) * split_size]\n",
        "        py = np.mean(part, axis=0)\n",
        "        kl_divs = part * (np.log(part + 1e-10) - np.log(py + 1e-10))\n",
        "        kl_div = np.mean(np.sum(kl_divs, axis=1))\n",
        "        scores.append(np.exp(kl_div))\n",
        "    \n",
        "    return np.mean(scores), np.std(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate VAE\n",
        "print(\"Evaluating VAE...\")\n",
        "NUM_EVAL_SAMPLES = 1000\n",
        "\n",
        "# Generate fake images\n",
        "print(f\"Generating {NUM_EVAL_SAMPLES} images...\")\n",
        "model.eval()\n",
        "fake_images = model.generate(NUM_EVAL_SAMPLES, device).cpu()\n",
        "\n",
        "# Collect real images\n",
        "print(\"Collecting real images...\")\n",
        "real_images = []\n",
        "for batch in test_loader:\n",
        "    real_images.append(batch)\n",
        "    if len(real_images) * batch.size(0) >= NUM_EVAL_SAMPLES:\n",
        "        break\n",
        "real_images = torch.cat(real_images, dim=0)[:NUM_EVAL_SAMPLES]\n",
        "\n",
        "print(f\"Real images shape: {real_images.shape}\")\n",
        "print(f\"Fake images shape: {fake_images.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate FID\n",
        "print(\"\\nCalculating FID Score...\")\n",
        "try:\n",
        "    inception_model = InceptionV3Features(device)\n",
        "    real_acts = get_activations(real_images, inception_model)\n",
        "    fake_acts = get_activations(fake_images, inception_model)\n",
        "    fid_score = calculate_fid(real_acts, fake_acts)\n",
        "    print(f\"FID Score: {fid_score:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"FID calculation failed: {e}\")\n",
        "    fid_score = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Inception Score\n",
        "print(\"\\nCalculating Inception Score...\")\n",
        "try:\n",
        "    is_mean, is_std = calculate_inception_score(fake_images, device)\n",
        "    print(f\"Inception Score: {is_mean:.4f} ± {is_std:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"IS calculation failed: {e}\")\n",
        "    is_mean, is_std = None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"FID Score: {fid_score:.4f}\" if fid_score else \"FID Score: N/A\")\n",
        "print(f\"Inception Score: {is_mean:.4f} ± {is_std:.4f}\" if is_mean else \"Inception Score: N/A\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save hyperparameters and results\n",
        "results_summary = f\"\"\"\n",
        "VAE Chest X-Ray Generation - Results Summary\n",
        "=============================================\n",
        "\n",
        "HYPERPARAMETERS:\n",
        "- Image Size: {CONFIG['img_size']}x{CONFIG['img_size']}\n",
        "- Latent Dimension: {CONFIG['latent_dim']}\n",
        "- Batch Size: {CONFIG['batch_size']}\n",
        "- Epochs: {CONFIG['epochs']}\n",
        "- Learning Rate: {CONFIG['learning_rate']}\n",
        "\n",
        "BETA ANNEALING:\n",
        "- Beta Start: {CONFIG['beta_start']}\n",
        "- Beta End: {CONFIG['beta_end']}\n",
        "- Warmup Epochs: {CONFIG['beta_warmup_epochs']}\n",
        "- Strategy: Linear annealing from {CONFIG['beta_start']} to {CONFIG['beta_end']} over {CONFIG['beta_warmup_epochs']} epochs\n",
        "\n",
        "TRAINING RESULTS:\n",
        "- Final Train Loss: {train_losses[-1]:.4f}\n",
        "- Final Val Loss: {val_losses[-1]:.4f}\n",
        "- Best Val Loss: {best_loss:.4f}\n",
        "- Final Reconstruction Loss: {recon_losses[-1]:.4f}\n",
        "- Final KL Loss: {kl_losses[-1]:.4f}\n",
        "\n",
        "EVALUATION METRICS:\n",
        "- FID Score: {fid_score:.4f if fid_score else 'N/A'}\n",
        "- Inception Score: {f'{is_mean:.4f} ± {is_std:.4f}' if is_mean else 'N/A'}\n",
        "\n",
        "MODEL ARCHITECTURE:\n",
        "- Encoder: 5 Conv layers (32->64->128->256->512) with BatchNorm\n",
        "- Decoder: 5 TransposeConv layers (512->256->128->64->32->1)\n",
        "- Total Parameters: {total_params:,}\n",
        "\"\"\"\n",
        "\n",
        "with open('/kaggle/working/results/summary.txt', 'w') as f:\n",
        "    f.write(results_summary)\n",
        "\n",
        "print(results_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all saved files\n",
        "print(\"\\nSaved files:\")\n",
        "print(\"\\nCheckpoints:\")\n",
        "for f in os.listdir('/kaggle/working/checkpoints'):\n",
        "    print(f\"  - {f}\")\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "for f in os.listdir('/kaggle/working/results'):\n",
        "    print(f\"  - {f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
